{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715},{"sourceId":7408860,"sourceType":"datasetVersion","datasetId":4309095}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## A. BASIC SENTIMENT ANALYSIS USING LOGISTIC REGRESSION","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load your dataset\n# Replace 'your_dataset.csv' with the path to your actual dataset\n# Dataset should have 'text' and 'sentiment' columns\ndf = pd.read_csv('/kaggle/input/sentiment-analysis/sentiment_analysis.csv')\n\n# Encode labels: positive=1, neutral=0, negative=-1\ndf['sentiment'] = df['sentiment'].map({'positive': 1, 'neutral': 0, 'negative': -1})\n\n# Split dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Convert text data to numerical data (Bag of Words)\nvectorizer = CountVectorizer()\nX_train_vectorized = vectorizer.fit_transform(X_train)\nX_test_vectorized = vectorizer.transform(X_test)\n\n# Build the Logistic Regression model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train_vectorized, y_train)\n\n# Predict on the test data\ny_pred = model.predict(X_test_vectorized)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy * 100:.2f}%')\nprint(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))\n\n# Example: Predict sentiment of new text\nnew_text = [\"I love this!\", \"It's okay, not the best.\", \"This is horrible.\"]\nnew_text_vectorized = vectorizer.transform(new_text)\npredictions = model.predict(new_text_vectorized)\nprint(\"Predictions (1 = positive, 0 = neutral, -1 = negative):\", predictions)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T15:49:38.847101Z","iopub.execute_input":"2024-09-25T15:49:38.847634Z","iopub.status.idle":"2024-09-25T15:49:39.069388Z","shell.execute_reply.started":"2024-09-25T15:49:38.847586Z","shell.execute_reply":"2024-09-25T15:49:39.067897Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Accuracy: 65.00%\n              precision    recall  f1-score   support\n\n    negative       0.75      0.58      0.66        36\n     neutral       0.48      0.80      0.60        30\n    positive       0.91      0.59      0.71        34\n\n    accuracy                           0.65       100\n   macro avg       0.71      0.66      0.66       100\nweighted avg       0.72      0.65      0.66       100\n\nPredictions (1 = positive, 0 = neutral, -1 = negative): [ 1 -1  0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## b. TWITTER SENTIMENT ANALYSIS USING LSTM AND GLOVE EMBEDDINGS","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom tqdm import tqdm\n\n# Check for GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load IMDb dataset (you can download the dataset from https://ai.stanford.edu/~amaas/data/sentiment/)\n# Assuming 'imdb_dataset.csv' with columns 'review' and 'sentiment' (positive=1, negative=0)\ndf = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Define a custom dataset class\nclass IMDbDataset(Dataset):\n    def __init__(self, reviews, sentiments, tokenizer, max_len):\n        self.reviews = reviews\n        self.sentiments = sentiments\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.reviews)\n    \n    def __getitem__(self, idx):\n        review = self.reviews.iloc[idx]\n        sentiment = self.sentiments.iloc[idx]\n        encoding = self.tokenizer.encode_plus(\n            review,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'review_text': review,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'sentiment': torch.tensor(sentiment, dtype=torch.long)\n        }\n\n# Load pre-trained BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Create data loaders\nMAX_LEN = 128\nBATCH_SIZE = 16\n\ntrain_dataset = IMDbDataset(X_train, y_train, tokenizer, MAX_LEN)\ntest_dataset = IMDbDataset(X_test, y_test, tokenizer, MAX_LEN)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\n# Load pre-trained BERT model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\nmodel = model.to(device)\n\n# Optimizer and learning rate scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Training function\ndef train_epoch(model, data_loader, optimizer, device):\n    model.train()\n    total_loss = 0\n    correct_predictions = 0\n    \n    for batch in tqdm(data_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        sentiments = batch['sentiment'].to(device)\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=sentiments)\n        loss = outputs.loss\n        logits = outputs.logits\n        \n        _, preds = torch.max(logits, dim=1)\n        correct_predictions += torch.sum(preds == sentiments)\n        total_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    return correct_predictions.double() / len(data_loader.dataset), total_loss / len(data_loader)\n\n# Evaluation function\ndef evaluate_model(model, data_loader, device):\n    model.eval()\n    total_loss = 0\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            sentiments = batch['sentiment'].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=sentiments)\n            loss = outputs.loss\n            logits = outputs.logits\n            \n            _, preds = torch.max(logits, dim=1)\n            correct_predictions += torch.sum(preds == sentiments)\n            total_loss += loss.item()\n    \n    return correct_predictions.double() / len(data_loader.dataset), total_loss / len(data_loader)\n\n# Training loop\nEPOCHS = 1\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    train_acc, train_loss = train_epoch(model, train_loader, optimizer, device)\n    print(f'Train loss {train_loss}, accuracy {train_acc}')\n    \n    test_acc, test_loss = evaluate_model(model, test_loader, device)\n    print(f'Test loss {test_loss}, accuracy {test_acc}')\n\n# Evaluate the model on test set\ny_pred = []\ny_true = []\n\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        sentiments = batch['sentiment'].to(device)\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        _, preds = torch.max(logits, dim=1)\n        \n        y_pred.extend(preds.cpu().numpy())\n        y_true.extend(sentiments.cpu().numpy())\n\n# Print classification report\nprint(classification_report(y_true, y_pred, target_names=['negative', 'positive']))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T16:22:43.170030Z","iopub.execute_input":"2024-09-25T16:22:43.170610Z","iopub.status.idle":"2024-09-25T16:41:47.046622Z","shell.execute_reply.started":"2024-09-25T16:22:43.170533Z","shell.execute_reply":"2024-09-25T16:41:47.045447Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2500/2500 [14:32<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.30962488082051276, accuracy 0.8657\nTest loss 0.2514863880723715, accuracy 0.8927\n              precision    recall  f1-score   support\n\n    negative       0.89      0.89      0.89      4961\n    positive       0.89      0.90      0.89      5039\n\n    accuracy                           0.89     10000\n   macro avg       0.89      0.89      0.89     10000\nweighted avg       0.89      0.89      0.89     10000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## C. MOVIE REVIEWS SENTIMENT CLASSIFICATION WITH BERT","metadata":{}},{"cell_type":"markdown","source":"Don't run the below....it will take time","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom tqdm import tqdm\n\n# Check for GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load IMDb dataset\n# Assuming 'imdb_dataset.csv' with columns 'review' and 'sentiment' (positive=1, negative=0)\ndf = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Define a custom dataset class\nclass IMDbDataset(Dataset):\n    def __init__(self, reviews, sentiments, tokenizer, max_len):\n        self.reviews = reviews\n        self.sentiments = sentiments\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.reviews)\n    \n    def __getitem__(self, idx):\n        review = self.reviews.iloc[idx]\n        sentiment = self.sentiments.iloc[idx]\n        encoding = self.tokenizer.encode_plus(\n            review,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'review_text': review,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'sentiment': torch.tensor(sentiment, dtype=torch.long)\n        }\n\n# Load pre-trained BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Create data loaders\nMAX_LEN = 128\nBATCH_SIZE = 8  # Reduced batch size to save memory\n\ntrain_dataset = IMDbDataset(X_train, y_train, tokenizer, MAX_LEN)\ntest_dataset = IMDbDataset(X_test, y_test, tokenizer, MAX_LEN)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\n# Load pre-trained BERT model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\nmodel = model.to(device)\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Gradient accumulation settings\naccumulation_steps = 4  # Adjust this value based on available memory\n\n# Training function\ndef train_epoch(model, data_loader, optimizer, device):\n    model.train()\n    total_loss = 0\n    correct_predictions = 0\n    \n    for i, batch in enumerate(tqdm(data_loader)):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        sentiments = batch['sentiment'].to(device)\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=sentiments)\n        loss = outputs.loss\n        \n        loss = loss / accumulation_steps  # Normalize loss for accumulation\n        loss.backward()  # Backpropagation\n        \n        if (i + 1) % accumulation_steps == 0:  # Update weights every 'accumulation_steps'\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        total_loss += loss.item()\n        correct_predictions += torch.sum(torch.argmax(outputs.logits, dim=1) == sentiments).item()\n    \n    return correct_predictions / len(data_loader.dataset), total_loss / len(data_loader)\n\n# Evaluation function\ndef evaluate_model(model, data_loader, device):\n    model.eval()\n    total_loss = 0\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            sentiments = batch['sentiment'].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=sentiments)\n            loss = outputs.loss\n            total_loss += loss.item()\n            correct_predictions += torch.sum(torch.argmax(outputs.logits, dim=1) == sentiments).item()\n    \n    return correct_predictions / len(data_loader.dataset), total_loss / len(data_loader)\n\n# Training loop\nEPOCHS = 1\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    train_acc, train_loss = train_epoch(model, train_loader, optimizer, device)\n    print(f'Train loss {train_loss}, accuracy {train_acc}')\n    \n    test_acc, test_loss = evaluate_model(model, test_loader, device)\n    print(f'Test loss {test_loss}, accuracy {test_acc}')\n\n# Evaluate the model on the test set\ny_pred = []\ny_true = []\n\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        sentiments = batch['sentiment'].to(device)\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        y_pred.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n        y_true.extend(sentiments.cpu().numpy())\n\n# Print classification report\nprint(classification_report(y_true, y_pred, target_names=['negative', 'positive']))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T15:55:07.741824Z","iopub.execute_input":"2024-09-25T15:55:07.742310Z","iopub.status.idle":"2024-09-25T16:11:30.539497Z","shell.execute_reply.started":"2024-09-25T15:55:07.742261Z","shell.execute_reply":"2024-09-25T16:11:30.537947Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ed26e347a30495691f7f4a3145ff2b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d5651e198514abc85a66aa4c4d4cb69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6511d03b45ce4148bc1aa4d4dc847cbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c3950acd25436a87c56f7c4d18dfe7"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"077f88616d31408780f73a466e9d85e2"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5000/5000 [15:26<00:00,  5.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.07945430947779678, accuracy 0.8621\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 129\u001b[0m\n\u001b[1;32m    126\u001b[0m     train_acc, train_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, device)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m     test_acc, test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n","Cell \u001b[0;32mIn[1], line 109\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m    106\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m    110\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    111\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36mIMDbDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 33\u001b[0m     review \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreviews\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     34\u001b[0m     sentiment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentiments\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[1;32m     35\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m     36\u001b[0m         review,\n\u001b[1;32m     37\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     44\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1683\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_integer\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39minteger, axis: AxisInt) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;124;03m    Check that 'key' is a valid position in the desired axis.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;124;03m        If 'key' is not a valid position in axis 'axis'.\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1683\u001b[0m     len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1684\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[1;32m   1685\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}