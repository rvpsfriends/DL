{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7685004,"sourceType":"datasetVersion","datasetId":4484220}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" a. BASIC MACHINE TRANSLATION USING RULE-BASED METHODS\n","metadata":{}},{"cell_type":"code","source":"#Step 1: Define the Bilingual Dictionary\n\n# Bilingual dictionary (English to French)\ndictionary = {\n    'hello': 'bonjour',\n    'world': 'monde',\n    'my': 'mon',\n    'name': 'nom',\n    'is': 'est',\n    'good': 'bon',\n    'morning': 'matin',\n    'thank': 'merci',\n    'you': 'vous',\n    'goodbye': 'au revoir'\n}\n\n# step 2 : Define Basic Grammar Rules\n\n# Basic grammar rule: Subject-Verb-Object (SVO)\ngrammar_rules = {\n    'SVO': ['subject', 'verb', 'object']\n}\n\n#Step 3: Translation Function\n\n\ndef translate(sentence):\n    # Convert sentence to lowercase and split it into words\n    words = sentence.lower().split()\n    \n    # Translate each word using the dictionary; if the word is not in the dictionary, keep it unchanged\n    translated_words = [dictionary.get(word, word) for word in words]\n    \n    # Join the translated words back into a sentence\n    return ' '.join(translated_words)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T15:54:41.667616Z","iopub.execute_input":"2024-10-06T15:54:41.668549Z","iopub.status.idle":"2024-10-06T15:54:41.675510Z","shell.execute_reply.started":"2024-10-06T15:54:41.668506Z","shell.execute_reply":"2024-10-06T15:54:41.674516Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Example usage\nsentence = \"Hello world\"\nprint(translate(sentence))  # Output: bonjour monde","metadata":{"execution":{"iopub.status.busy":"2024-10-06T15:54:49.458127Z","iopub.execute_input":"2024-10-06T15:54:49.458863Z","iopub.status.idle":"2024-10-06T15:54:49.463833Z","shell.execute_reply.started":"2024-10-06T15:54:49.458821Z","shell.execute_reply":"2024-10-06T15:54:49.462843Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"bonjour monde\n","output_type":"stream"}]},{"cell_type":"code","source":"sentence2 = \"Good morning\"\nprint(translate(sentence2))","metadata":{"execution":{"iopub.status.busy":"2024-10-06T15:55:00.624166Z","iopub.execute_input":"2024-10-06T15:55:00.624603Z","iopub.status.idle":"2024-10-06T15:55:00.630297Z","shell.execute_reply.started":"2024-10-06T15:55:00.624556Z","shell.execute_reply":"2024-10-06T15:55:00.629153Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"bon matin\n","output_type":"stream"}]},{"cell_type":"code","source":"sentence3 = \"Thank you\"\nprint(translate(sentence3)) ","metadata":{"execution":{"iopub.status.busy":"2024-10-06T15:55:11.469242Z","iopub.execute_input":"2024-10-06T15:55:11.469874Z","iopub.status.idle":"2024-10-06T15:55:11.474486Z","shell.execute_reply.started":"2024-10-06T15:55:11.469830Z","shell.execute_reply":"2024-10-06T15:55:11.473537Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"merci vous\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\nb. ENGLISH TO FRENCH TRANSLATION USING SEQ2SEQ WITH ATTENTION","metadata":{}},{"cell_type":"code","source":"! pip install --upgrade tensorflow-datasets","metadata":{"execution":{"iopub.status.busy":"2024-10-06T15:55:58.869289Z","iopub.execute_input":"2024-10-06T15:55:58.869710Z","iopub.status.idle":"2024-10-06T15:56:10.890135Z","shell.execute_reply.started":"2024-10-06T15:55:58.869669Z","shell.execute_reply":"2024-10-06T15:56:10.888927Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (4.9.6)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (1.4.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (8.1.7)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.1.8)\nRequirement already satisfied: immutabledict in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (4.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (1.26.4)\nRequirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (2.3)\nRequirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (3.20.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (5.9.3)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (16.1.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (2.32.3)\nRequirement already satisfied: simple-parsing in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.1.5)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.14.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (2.4.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.10.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (4.66.4)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (1.16.0)\nRequirement already satisfied: array-record>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.5.1)\nRequirement already satisfied: etils>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (1.7.0)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (4.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (2024.6.1)\nRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (6.4.0)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (3.19.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.8.30)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from promise->tensorflow-datasets) (1.16.0)\nRequirement already satisfied: docstring-parser~=0.15 in /opt/conda/lib/python3.10/site-packages (from simple-parsing->tensorflow-datasets) (0.16)\nRequirement already satisfied: googleapis-common-protos in /opt/conda/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.63.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport tensorflow_datasets as tfds\n\n# Step 1: Load dataset from CSV using Pandas\ndata_path = '/kaggle/input/wmt-2014-english-french/wmt14_translate_fr-en_test.csv'\ndata = pd.read_csv(data_path)\n\n# Check the first few rows and the column names of the dataframe\nprint(data.head())\nprint(\"Columns in the DataFrame:\", data.columns.tolist())  # Print the actual column names\n\n# Ensure the dataframe contains the required columns\nexpected_columns = ['en', 'fr']\nassert all(col in data.columns for col in expected_columns), f\"CSV must contain {expected_columns} columns\"\n\n# Step 2: Convert the DataFrame to a TensorFlow Dataset\n# Create a TensorFlow dataset from the DataFrame\ntrain_dataset = tf.data.Dataset.from_tensor_slices((data['en'].values, data['fr'].values))\n\n# Print the first example to verify conversion\nfor english, french in train_dataset.take(1):\n    print(f'English: {english.numpy().decode(\"utf-8\")}, French: {french.numpy().decode(\"utf-8\")}')\n\n# Optional: Define constants for batch size and max length\nBATCH_SIZE = 64\nMAX_LENGTH = 40\n\n# Optional: Tokenization process\n# Tokenizer setup for input (English) and output (French)\ntokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    (en.numpy() for en, fr in train_dataset), target_vocab_size=2**13)\ntokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    (fr.numpy() for en, fr in train_dataset), target_vocab_size=2**13)\n\n# Encoding function\ndef encode(en_t, fr_t):\n    en_t = [tokenizer_en.vocab_size] + tokenizer_en.encode(en_t.numpy().decode('utf-8')) + [tokenizer_en.vocab_size + 1]\n    fr_t = [tokenizer_fr.vocab_size] + tokenizer_fr.encode(fr_t.numpy().decode('utf-8')) + [tokenizer_fr.vocab_size + 1]\n    return en_t, fr_t\n\ndef tf_encode(en_t, fr_t):\n    return tf.py_function(encode, [en_t, fr_t], [tf.int64, tf.int64])\n\n# Prepare the dataset with encoding\ntrain_dataset = train_dataset.map(tf_encode)\n\n# Filter sequences longer than MAX_LENGTH\ndef filter_max_length(en, fr, max_length=MAX_LENGTH):\n    return tf.logical_and(tf.size(en) <= max_length, tf.size(fr) <= max_length)\n\ntrain_dataset = train_dataset.filter(filter_max_length)\n\n# Shuffle and batch the dataset\ntrain_dataset = train_dataset.shuffle(20000).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n# Print the first training example after processing\nfor en, fr in train_dataset.take(1):\n    print(f'Encoded English: {en.numpy()}')\n    print(f'Encoded French: {fr.numpy()}')","metadata":{"execution":{"iopub.status.busy":"2024-10-06T16:07:18.588960Z","iopub.execute_input":"2024-10-06T16:07:18.589890Z","iopub.status.idle":"2024-10-06T16:07:59.121367Z","shell.execute_reply.started":"2024-10-06T16:07:18.589845Z","shell.execute_reply":"2024-10-06T16:07:59.120122Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"                                                  en  \\\n0              Spectacular Wingsuit Jump Over Bogota   \n1  Sportsman Jhonathan Florez jumped from a helic...   \n2  Wearing a wingsuit, he flew past over the famo...   \n3                           A black box in your car?   \n4  As America's road planners struggle to find th...   \n\n                                                  fr  \n0  Spectaculaire saut en \"wingsuit\" au-dessus de ...  \n1  Le sportif Jhonathan Florez a sauté jeudi d'un...  \n2  Equipé d'un wingsuit (une combinaison munie d'...  \n3               Une boîte noire dans votre voiture ?  \n4  Alors que les planificateurs du réseau routier...  \nColumns in the DataFrame: ['en', 'fr']\nEnglish: Spectacular Wingsuit Jump Over Bogota, French: Spectaculaire saut en \"wingsuit\" au-dessus de Bogota\nEncoded English: [[7639 7417   31 ...    0    0    0]\n [7639  587  626 ...    0    0    0]\n [7639   79   14 ...    0    0    0]\n ...\n [7639 3174   37 ...    0    0    0]\n [7639  533 2463 ...    0    0    0]\n [7639 1850    2 ...    0    0    0]]\nEncoded French: [[8256   99  213 ...    0    0    0]\n [8256  969  379 ...    0    0    0]\n [8256   44 5782 ...    0    0    0]\n ...\n [8256 1876  692 ...    0    0    0]\n [8256   35 1357 ...    0    0    0]\n [8256 1536    2 ...    0    0    0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}